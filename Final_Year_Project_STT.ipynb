{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLPvLtaPOWEE"
      },
      "source": [
        "# **Data Preprocessing/Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC2tiTcUOfqW"
      },
      "source": [
        "####**Installing Dependecies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__25aAzoRVRH",
        "outputId": "acd0919c-7928-497e-d2d0-b761f9fd29cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://storage.googleapis.com/tpu-pytorch/wheels/colab.html\n",
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.21.0-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-2.6.0-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torch_xla[tpu] (from versions: none)\n",
            "ERROR: No matching distribution found for torch_xla[tpu]\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opencv-python\n",
            "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\neche\\documents\\academics at tu\\400 level\\final year proj\\stt converter\\whisper-env\\lib\\site-packages (from opencv-python) (2.2.4)\n",
            "Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
            "Installing collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.11.0.86\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "ERROR: Could not find a version that satisfies the requirement google-colab (from versions: none)\n",
            "ERROR: No matching distribution found for google-colab\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio torch_xla[tpu] -f https://storage.googleapis.com/tpu-pytorch/wheels/colab.html\n",
        "!pip install opencv-python\n",
        "!pip install google-colab\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install soundfile\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install librosa\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdpmK6g_Ol5J"
      },
      "source": [
        "####**Preprocessing The Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt-mDySYP_2H"
      },
      "source": [
        "*The dataset consists of this languages ;*\n",
        "\n",
        "\n",
        "*   \"ig\"  -     **Igbo**\n",
        "*   \"ha\" - **Hausa**\n",
        "*   \"yo\" - **Yoruba**\n",
        "*   \"en\" - **English**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7vf7oZeKdD5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch_xla.core.xla_model as xm\n",
        "from datasets import load_dataset\n",
        "from google.colab import userdata\n",
        "\n",
        "# === 1Ô∏è‚É£ Authenticate with Hugging Face ===\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")  # Retrieve token\n",
        "if not HF_TOKEN:\n",
        "    raise ValueError(\"‚ùå Hugging Face token not found in Colab userdata.\")\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # Set environment variable\n",
        "\n",
        "# === 2Ô∏è‚É£ Define Directories ===\n",
        "BASE_DIR = \"/content/preprocessed_languages_datasets\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# === 3Ô∏è‚É£ Define Languages ===\n",
        "LANGUAGES = [\"en\", \"yo\", \"ha\", \"ig\"]\n",
        "\n",
        "# === 4Ô∏è‚É£ Enable TPU ===\n",
        "device = xm.xla_device()\n",
        "print(f\"‚úÖ Using device: {device}\")\n",
        "\n",
        "# === 5Ô∏è‚É£ Define Special Characters and Contractions ===\n",
        "special_characters = {\n",
        "    \"yo\": \"√†√®√©·∫π·ªç√π√∫≈Ñ·π£√°√©·∫π·ªç√π√∫≈Ñ·π£\",\n",
        "    \"ha\": \"…ì…ó∆ô∆¥∆Å∆ä∆ò∆≥\",\n",
        "    \"ig\": \"·πÖ·ªç·ª•·ªã·ªã·ª•·ªç≈Ñ≈Ñ\",\n",
        "    \"fr\": \"√ß√†√©√®√™√¥√π√ª√¢√´√Ø√º√¶≈ì√ø√©√®√™√´√Æ√Ø√¥√ª√π√†√¢√ß\"\n",
        "}\n",
        "contractions = {\n",
        "    \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\",\n",
        "    \"i'll\": \"i will\", \"you'll\": \"you will\", \"he'll\": \"he will\",\n",
        "    \"she'll\": \"she will\", \"it'll\": \"it will\", \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\", \"they've\": \"they have\", \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
        "    \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\",\n",
        "    \"won't\": \"will not\", \"wouldn't\": \"would not\", \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\", \"didn't\": \"did not\", \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\", \"shouldn't\": \"should not\",\n",
        "    \"mightn't\": \"might not\", \"mustn't\": \"must not\"\n",
        "}\n",
        "\n",
        "# === 6Ô∏è‚É£ Text Preprocessing ===\n",
        "def preprocess_text(text, lang):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    if lang == \"en\":\n",
        "        for contraction, expanded in contractions.items():\n",
        "            text = text.replace(contraction, expanded)\n",
        "\n",
        "    allowed_chars = special_characters.get(lang, \"\") + \"abcdefghijklmnopqrstuvwxyz.,?!' \"\n",
        "    return \"\".join(c for c in text if c in allowed_chars).strip()\n",
        "\n",
        "# === 7Ô∏è‚É£ Preprocessing Function ===\n",
        "def load_and_preprocess_dataset(lang):\n",
        "    print(f\"\\nüöÄ Processing {lang.upper()}...\")\n",
        "    dataset_path = os.path.join(BASE_DIR, lang)\n",
        "    if os.path.exists(dataset_path):\n",
        "        shutil.rmtree(dataset_path)\n",
        "\n",
        "    dataset = load_dataset(\"mozilla-foundation/common_voice_13_0\", lang, token=HF_TOKEN, cache_dir=\"/content/dataset_cache\")\n",
        "    print(f\"‚úÖ Loaded splits: {list(dataset.keys())}\")\n",
        "\n",
        "    def preprocess_batch(batch):\n",
        "        batch[\"sentence\"] = preprocess_text(batch[\"sentence\"], lang)\n",
        "        return batch\n",
        "\n",
        "    dataset = dataset.map(preprocess_batch, desc=f\"Preprocessing {lang} dataset\")\n",
        "    dataset.save_to_disk(dataset_path)\n",
        "    print(f\"‚úÖ Saved preprocessed dataset to {dataset_path}\")\n",
        "\n",
        "# === 8Ô∏è‚É£ Run Preprocessing for All Languages ===\n",
        "for lang in LANGUAGES:\n",
        "    load_and_preprocess_dataset(lang)\n",
        "\n",
        "print(\"üéâ All datasets preprocessed and saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO52hs37u1W7"
      },
      "source": [
        "# **MODEL FINE-TUNING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpmPP1frp11d"
      },
      "source": [
        "####**Fine-tuning The Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlDDS6iiZcTu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "from datasets import load_from_disk\n",
        "import evaluate\n",
        "from google.colab import drive\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "import time\n",
        "\n",
        "# === Mount Google Drive ===\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Languages to Train ===\n",
        "LANGUAGES = [\"yo\", \"ha\", \"ig\", \"en\"]\n",
        "\n",
        "# === Paths & Config ===\n",
        "DATA_DIR = \"/content/drive/My Drive/Colab Notebooks/CommonVoice/preprocessed_languages_datasets\"\n",
        "OUTPUT_DIR = \"/content/whisper_models\"\n",
        "FINAL_MODEL_DIR = \"/content/drive/My Drive/Colab Notebooks/Models/whisper_multilingual\"\n",
        "\n",
        "LANG_TAGS = {\n",
        "    \"yo\": \"<|yo|>\",\n",
        "    \"ha\": \"<|ha|>\",\n",
        "    \"ig\": \"<|ig|>\",\n",
        "    \"en\": \"<|en|>\",\n",
        "}\n",
        "\n",
        "# === Clean Conflicts ===\n",
        "conflict_paths = [\"/content/whisper-base\"]\n",
        "for path in conflict_paths:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "        print(f\"Deleted conflicting folder: {path}\")\n",
        "\n",
        "# === Metric ===\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "# === Data Collator ===\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: WhisperProcessor\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
        "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "        batch[\"labels\"] = labels_batch[\"input_ids\"]\n",
        "        return batch\n",
        "\n",
        "# === Training Args ===\n",
        "def get_training_args(lang):\n",
        "    return Seq2SeqTrainingArguments(\n",
        "        output_dir=f\"{OUTPUT_DIR}/{lang}\",\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=2,\n",
        "        logging_steps=500,\n",
        "        eval_steps=500,\n",
        "        save_steps=1000,\n",
        "        learning_rate=3e-4,\n",
        "        weight_decay=0.005,\n",
        "        num_train_epochs=3,\n",
        "        logging_dir=f\"{OUTPUT_DIR}/{lang}/logs\",\n",
        "        save_total_limit=2,\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=128,\n",
        "        dataloader_num_workers=1,\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "# === Dataset Preprocessing ===\n",
        "def prepare_dataset(lang, processor, split=\"train\"):\n",
        "    dataset = load_from_disk(f\"{DATA_DIR}/{lang}/{split}\")\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=16000)\n",
        "\n",
        "    def preprocess_batch(batch):\n",
        "        audio = batch[\"audio\"]\n",
        "        waveform = torch.tensor(audio[\"array\"], dtype=torch.float32).unsqueeze(0)\n",
        "        resampled = resampler(waveform).squeeze().numpy()\n",
        "        inputs = processor(resampled, sampling_rate=16000)\n",
        "        batch[\"input_features\"] = inputs[\"input_features\"][0]\n",
        "        sentence = LANG_TAGS[lang] + \" \" + batch[\"sentence\"]\n",
        "        batch[\"labels\"] = processor.tokenizer(sentence).input_ids\n",
        "        return batch\n",
        "\n",
        "    dataset = dataset.map(preprocess_batch, remove_columns=dataset.column_names)\n",
        "    dataset = dataset.filter(lambda x: x[\"input_features\"] is not None and len(x[\"labels\"]) > 0)\n",
        "    dataset.set_format(type=\"torch\")\n",
        "    return dataset\n",
        "\n",
        "# === Metrics ===\n",
        "def compute_metrics(eval_pred):\n",
        "    pred_ids = eval_pred.predictions\n",
        "    label_ids = eval_pred.label_ids\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    return {\"wer\": wer_metric.compute(predictions=pred_str, references=label_str)}\n",
        "\n",
        "# === Loop Over Languages ===\n",
        "for lang in LANGUAGES:\n",
        "    print(f\"\\nüöÄ Starting training for: {lang}\")\n",
        "    try:\n",
        "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\", device_map=\"auto\")\n",
        "\n",
        "        train_dataset = prepare_dataset(lang, processor, split=\"train\")\n",
        "\n",
        "        if len(train_dataset) == 0:\n",
        "            print(f\"‚ö†Ô∏è No training data for {lang}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "        training_args = get_training_args(lang)\n",
        "\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            tokenizer=processor,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        # Evaluate\n",
        "        print(f\"\\nüîç Evaluating model for: {lang}\")\n",
        "        test_dataset = prepare_dataset(lang, processor, split=\"test\")\n",
        "        eval_results = trainer.evaluate(test_dataset)\n",
        "        print(f\"‚úÖ Evaluation Results ({lang}):\", eval_results)\n",
        "\n",
        "        # Save\n",
        "        model.save_pretrained(f\"{FINAL_MODEL_DIR}/{lang}\")\n",
        "        processor.save_pretrained(f\"{FINAL_MODEL_DIR}/{lang}\")\n",
        "        print(f\"‚úÖ Model saved for: {lang}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error occurred while training {lang}: {e}\")\n",
        "\n",
        "    # Clear memory\n",
        "    del model, processor, trainer, train_dataset\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(10)\n",
        "    print(f\"‚úÖ Finished {lang}, GPU memory cleared. Moving to next language...\\n{'='*50}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "whisper-env",
      "language": "python",
      "name": "whisper-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
